{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "    vocab = defaultdict(int)\n",
    "    for word in data:\n",
    "        vocab[' '.join(word) + ' </w>'] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in v_in:\n",
    "        w_out = word.replace(bigram, replacement)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_pair_encoding(data, num_merges):\n",
    "    vocab = get_vocab(data)\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, vocab):\n",
    "    sentence = list(sentence) + ['</w>']\n",
    "    tokens = []\n",
    "    while sentence:\n",
    "        found = False\n",
    "        for i in range(len(sentence) - 1, 0, -1):\n",
    "            pair = ' '.join(sentence[i - 1:i + 1])\n",
    "            if pair in vocab:\n",
    "                tokens.append(pair)\n",
    "                sentence = sentence[:i - 1] + [pair] + sentence[i + 1:]\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            tokens.append(sentence[0])\n",
    "            sentence = sentence[1:]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'low </w>': 1, 'low e s t </w>': 1, 'new er </w>': 1, 'w i d er </w>': 1, 'new </w>': 1}\n",
      "Tokenized Sentence: ['l', 'o', 'w', 'e', 'r', '</w>']\n"
     ]
    }
   ],
   "source": [
    "data = [\"low\", \"lowest\", \"newer\", \"wider\", \"new\"]\n",
    "vocab = byte_pair_encoding(data, 5)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "sentence = \"lower\"\n",
    "tokens = tokenize_sentence(sentence, vocab)\n",
    "print(\"Tokenized Sentence:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
